{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO_YAD2K.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEzglolqODb3",
        "colab_type": "code",
        "outputId": "1dafb405-0cdd-415b-d5f9-f48719605f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olRdKsC1Lrb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lEJxqO3OM0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/WIDER/ /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hncmLvuOTsP",
        "colab_type": "code",
        "outputId": "cdf1da0a-9c20-4c1f-d384-5507ed034bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir data\n",
        "%cd data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVygkiqWOcE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!unzip /content/gdrive/My\\ Drive/WIDER/WIDER_train.zip\n",
        "!unzip /content/gdrive/My\\ Drive//WIDER/WIDER_test.zip\n",
        "!unzip /content/gdrive/My\\ Drive//WIDER/WIDER_val.zip\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpBOcHV0OjB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import shuffle\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "import io\n",
        "import pickle\n",
        "import os.path\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3tlU6CdPEwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_colors_for_classes(num_classes):\n",
        "    \"\"\"Return list of random colors for number of classes given.\"\"\"\n",
        "    # Use previously generated colors if num_classes is the same.\n",
        "    if (hasattr(get_colors_for_classes, \"colors\") and\n",
        "            len(get_colors_for_classes.colors) == num_classes):\n",
        "        return get_colors_for_classes.colors\n",
        "\n",
        "    hsv_tuples = [(x / num_classes, 1., 1.) for x in range(num_classes)]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(\n",
        "        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
        "            colors))\n",
        "    random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
        "    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "    random.seed(None)  # Reset seed to default.\n",
        "    get_colors_for_classes.colors = colors  # Save colors for future calls.\n",
        "    return colors\n",
        "\n",
        "\n",
        "def draw_boxes_(image, boxes, box_classes, class_names, scores=None):\n",
        "   \n",
        "    image = Image.fromarray(np.floor(image * 255 + 0.5).astype('uint8'))\n",
        "\n",
        "    font = ImageFont.truetype(\n",
        "        font='/content/YAD2K/font/FiraMono-Medium.otf',\n",
        "        size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "    thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "    colors = get_colors_for_classes(len(class_names))\n",
        "\n",
        "    for i, c in list(enumerate(box_classes)):\n",
        "        box_class = class_names[c]\n",
        "        box = boxes[i]\n",
        "        if isinstance(scores, np.ndarray):\n",
        "            score = scores[i]\n",
        "            label = '{} {:.2f}'.format(box_class, score)\n",
        "        else:\n",
        "            label = '{}'.format(box_class)\n",
        "\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        label_size = draw.textsize(label, font)\n",
        "\n",
        "        top, left, bottom, right = box\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "        print(label, (left, top), (right, bottom))\n",
        "\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle(\n",
        "                [left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
        "        draw.rectangle(\n",
        "            [tuple(text_origin), tuple(text_origin + label_size)],\n",
        "            fill=colors[c])\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        del draw\n",
        "\n",
        "    return np.array(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7du7QMfuPQgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_classes(classes_path):\n",
        "    '''loads the classes'''\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def get_anchors(anchors_path):\n",
        "    '''loads the anchors from a file'''\n",
        "    if os.path.isfile(anchors_path):\n",
        "        with open(anchors_path) as f:\n",
        "            anchors = f.readline()\n",
        "            anchors = [float(x) for x in anchors.split(',')]\n",
        "            return np.array(anchors).reshape(-1, 2)\n",
        "    else:\n",
        "        Warning(\"Could not open anchors file, using default.\")\n",
        "        return YOLO_ANCHORS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9U7-un8Omah",
        "colab_type": "code",
        "outputId": "bd976678-950a-44ed-8c3e-048df60105a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/allanzelener/YAD2K.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'YAD2K'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "Receiving objects:   0% (1/243)   \rReceiving objects:   1% (3/243)   \rReceiving objects:   2% (5/243)   \rReceiving objects:   3% (8/243)   \rReceiving objects:   4% (10/243)   \rReceiving objects:   5% (13/243)   \rReceiving objects:   6% (15/243)   \rReceiving objects:   7% (18/243)   \rReceiving objects:   8% (20/243)   \rReceiving objects:   9% (22/243)   \rReceiving objects:  10% (25/243)   \rReceiving objects:  11% (27/243)   \rReceiving objects:  12% (30/243)   \rReceiving objects:  13% (32/243)   \rReceiving objects:  14% (35/243)   \rReceiving objects:  15% (37/243)   \rReceiving objects:  16% (39/243)   \rReceiving objects:  17% (42/243)   \rReceiving objects:  18% (44/243)   \rReceiving objects:  19% (47/243)   \rReceiving objects:  20% (49/243)   \rReceiving objects:  21% (52/243)   \rReceiving objects:  22% (54/243)   \rReceiving objects:  23% (56/243)   \rReceiving objects:  24% (59/243)   \rReceiving objects:  25% (61/243)   \rReceiving objects:  26% (64/243)   \rReceiving objects:  27% (66/243)   \rReceiving objects:  28% (69/243)   \rReceiving objects:  29% (71/243)   \rReceiving objects:  30% (73/243)   \rReceiving objects:  31% (76/243)   \rReceiving objects:  32% (78/243)   \rReceiving objects:  33% (81/243)   \rReceiving objects:  34% (83/243)   \rReceiving objects:  35% (86/243)   \rReceiving objects:  36% (88/243)   \rReceiving objects:  37% (90/243)   \rReceiving objects:  38% (93/243)   \rReceiving objects:  39% (95/243)   \rReceiving objects:  40% (98/243)   \rReceiving objects:  41% (100/243)   \rReceiving objects:  42% (103/243)   \rReceiving objects:  43% (105/243)   \rReceiving objects:  44% (107/243)   \rReceiving objects:  45% (110/243)   \rReceiving objects:  46% (112/243)   \rReceiving objects:  47% (115/243)   \rReceiving objects:  48% (117/243)   \rReceiving objects:  49% (120/243)   \rReceiving objects:  50% (122/243)   \rReceiving objects:  51% (124/243)   \rReceiving objects:  52% (127/243)   \rReceiving objects:  53% (129/243)   \rReceiving objects:  54% (132/243)   \rReceiving objects:  55% (134/243)   \rReceiving objects:  56% (137/243)   \rReceiving objects:  57% (139/243)   \rReceiving objects:  58% (141/243)   \rReceiving objects:  59% (144/243)   \rReceiving objects:  60% (146/243)   \rReceiving objects:  61% (149/243)   \rReceiving objects:  62% (151/243)   \rReceiving objects:  63% (154/243)   \rReceiving objects:  64% (156/243)   \rReceiving objects:  65% (158/243)   \rReceiving objects:  66% (161/243)   \rReceiving objects:  67% (163/243)   \rReceiving objects:  68% (166/243)   \rReceiving objects:  69% (168/243)   \rReceiving objects:  70% (171/243)   \rReceiving objects:  71% (173/243)   \rReceiving objects:  72% (175/243)   \rReceiving objects:  73% (178/243)   \rReceiving objects:  74% (180/243)   \rReceiving objects:  75% (183/243)   \rReceiving objects:  76% (185/243)   \rReceiving objects:  77% (188/243)   \rReceiving objects:  78% (190/243)   \rReceiving objects:  79% (192/243)   \rReceiving objects:  80% (195/243)   \rReceiving objects:  81% (197/243)   \rReceiving objects:  82% (200/243)   \rReceiving objects:  83% (202/243)   \rReceiving objects:  84% (205/243)   \rReceiving objects:  85% (207/243)   \rReceiving objects:  86% (209/243)   \rReceiving objects:  87% (212/243)   \rReceiving objects:  88% (214/243)   \rReceiving objects:  89% (217/243)   \rReceiving objects:  90% (219/243)   \rReceiving objects:  91% (222/243)   \rReceiving objects:  92% (224/243)   \rReceiving objects:  93% (226/243)   \rremote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects:  94% (229/243)   \rReceiving objects:  95% (231/243)   \rReceiving objects:  96% (234/243)   \rReceiving objects:  97% (236/243)   \rReceiving objects:  98% (239/243)   \rReceiving objects:  99% (241/243)   \rReceiving objects: 100% (243/243)   \rReceiving objects: 100% (243/243), 2.35 MiB | 13.99 MiB/s, done.\n",
            "Resolving deltas:   0% (0/106)   \rResolving deltas:   2% (3/106)   \rResolving deltas:   3% (4/106)   \rResolving deltas:   7% (8/106)   \rResolving deltas:  16% (17/106)   \rResolving deltas:  17% (19/106)   \rResolving deltas:  19% (21/106)   \rResolving deltas:  22% (24/106)   \rResolving deltas:  23% (25/106)   \rResolving deltas:  27% (29/106)   \rResolving deltas:  28% (30/106)   \rResolving deltas:  33% (35/106)   \rResolving deltas:  35% (38/106)   \rResolving deltas:  46% (49/106)   \rResolving deltas:  54% (58/106)   \rResolving deltas:  55% (59/106)   \rResolving deltas:  59% (63/106)   \rResolving deltas:  66% (71/106)   \rResolving deltas:  84% (90/106)   \rResolving deltas:  87% (93/106)   \rResolving deltas:  89% (95/106)   \rResolving deltas:  91% (97/106)   \rResolving deltas:  92% (98/106)   \rResolving deltas:  95% (101/106)   \rResolving deltas: 100% (106/106)   \rResolving deltas: 100% (106/106), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI3Cb13hOyrv",
        "colab_type": "code",
        "outputId": "ce2a7e96-0117-4949-8ca1-357e79437185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://gitlab.aiacademy.tw/aai/examples/raw/a71e50c55115627bd5d8ed2ddc5f22d26e131836/yolo_data/model_data/yolo_topless.h5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-30 08:16:20--  https://gitlab.aiacademy.tw/aai/examples/raw/a71e50c55115627bd5d8ed2ddc5f22d26e131836/yolo_data/model_data/yolo_topless.h5\n",
            "Resolving gitlab.aiacademy.tw (gitlab.aiacademy.tw)... 35.236.148.211\n",
            "Connecting to gitlab.aiacademy.tw (gitlab.aiacademy.tw)|35.236.148.211|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202360872 (193M) [application/octet-stream]\n",
            "Saving to: ‘yolo_topless.h5’\n",
            "\n",
            "yolo_topless.h5     100%[===================>] 192.99M  11.2MB/s    in 19s     \n",
            "\n",
            "2019-04-30 08:16:42 (10.3 MB/s) - ‘yolo_topless.h5’ saved [202360872/202360872]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cpOmOYQO3uw",
        "colab_type": "code",
        "outputId": "394d0e9a-a01d-42f2-bd54-b3e260bae276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://gitlab.aiacademy.tw/aai/examples/raw/a71e50c55115627bd5d8ed2ddc5f22d26e131836/yolo_data/model_data/yolo.h5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-30 08:16:45--  https://gitlab.aiacademy.tw/aai/examples/raw/a71e50c55115627bd5d8ed2ddc5f22d26e131836/yolo_data/model_data/yolo.h5\n",
            "Resolving gitlab.aiacademy.tw (gitlab.aiacademy.tw)... 35.236.148.211\n",
            "Connecting to gitlab.aiacademy.tw (gitlab.aiacademy.tw)|35.236.148.211|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204146732 (195M) [application/octet-stream]\n",
            "Saving to: ‘yolo.h5’\n",
            "\n",
            "yolo.h5             100%[===================>] 194.69M  13.8MB/s    in 16s     \n",
            "\n",
            "2019-04-30 08:17:06 (12.4 MB/s) - ‘yolo.h5’ saved [204146732/204146732]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za3_isT3Y3Fj",
        "colab_type": "code",
        "outputId": "bd43214b-ae3f-4a7c-fdba-80116be7e678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import colorsys\n",
        "import random\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from keras import optimizers as op\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Lambda, Conv2D\n",
        "from keras.models import load_model, Model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from YAD2K.yad2k.models.keras_yolo import (preprocess_true_boxes, yolo_body,\n",
        "                                     yolo_eval, yolo_head, yolo_loss)\n",
        "from YAD2K.yad2k.utils.draw_boxes import draw_boxes\n",
        "\n",
        "\n",
        "# Default anchor boxes\n",
        "YOLO_ANCHORS = np.array(\n",
        "    ((0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434),\n",
        "     (7.88282, 3.52778), (9.77052, 9.16828)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54tHgLObRp6R",
        "colab_type": "code",
        "outputId": "489420c5-51d6-4288-b1e9-bf66a1a020df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/WIDER/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WIDER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiNLhtnxO96Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_box = np.load('final_boxes_preprocess.npz',allow_pickle=True)\n",
        "train_det = np.load('detectandmatch.npz',allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEEIHN_HRsaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_box = np.load('val_final_boxes_preprocess.npz',allow_pickle = True)\n",
        "val_det = np.load('val_detectandmatch.npz',allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjKQr6UGS0Y8",
        "colab_type": "code",
        "outputId": "23526766-153a-4e2f-a494-e5092c969e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_box[\"boxes\"].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3225, 709, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u_6UF5tRwHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_box(boxes, batch_size = 5,mode = \"train\"):\n",
        "  i =0\n",
        "  while True:\n",
        "    bb = []\n",
        "    while len(bb) < batch_size :\n",
        "      a = boxes[i]\n",
        "      i = i+1\n",
        "      #print(len(bb))\n",
        "      if(i == boxes.shape[0]):\n",
        "        i =0;\n",
        "        if mode == \"eval\":\n",
        "          break;\n",
        "      bb.append(a)\n",
        "\n",
        "    yield(np.array(bb))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3YsKc5_SJVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "box_batch_train_27 = batch_box(train_box[\"boxes\"], 27)\n",
        "box_batch_train_9 = batch_box(train_box[\"boxes\"], 9)\n",
        "box_batch_train_3 = batch_box(train_box[\"boxes\"], 3)\n",
        "box_batch_train_1 = batch_box(train_box[\"boxes\"], 1)\n",
        "\n",
        "det_batch_train_27 = batch_box(train_det[\"detectors_mask\"],27)\n",
        "det_batch_train_9 = batch_box(train_det[\"detectors_mask\"],9)\n",
        "det_batch_train_3 = batch_box(train_det[\"detectors_mask\"],3)\n",
        "det_batch_train_1 = batch_box(train_det[\"detectors_mask\"],1)\n",
        "\n",
        "match_batch_train_27 = batch_box(train_det[\"matching_true_boxes\"],27)\n",
        "match_batch_train_9 = batch_box(train_det[\"matching_true_boxes\"],9)\n",
        "match_batch_train_3 = batch_box(train_det[\"matching_true_boxes\"],3)\n",
        "match_batch_train_1 = batch_box(train_det[\"matching_true_boxes\"],1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCXfUZUzSMV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "box_batch_val_25 = batch_box(val_box[\"boxes\"], 25)\n",
        "box_batch_val_5 = batch_box(val_box[\"boxes\"], 5)\n",
        "box_batch_val_3 = batch_box(val_box[\"boxes\"], 3)\n",
        "box_batch_val_1 = batch_box(val_box[\"boxes\"], 1)\n",
        "\n",
        "det_batch_val_25 = batch_box(val_det[\"detectors_mask\"],25)\n",
        "det_batch_val_5 = batch_box(val_det[\"detectors_mask\"],5)\n",
        "det_batch_val_3 = batch_box(val_det[\"detectors_mask\"],3)\n",
        "det_batch_val_1 = batch_box(val_det[\"detectors_mask\"],1)\n",
        "\n",
        "match_batch_val_25= batch_box(val_det[\"matching_true_boxes\"],25)\n",
        "match_batch_val_5 = batch_box(val_det[\"matching_true_boxes\"],5)\n",
        "match_batch_val_3 = batch_box(val_det[\"matching_true_boxes\"],3)\n",
        "match_batch_val_1 = batch_box(val_det[\"matching_true_boxes\"],1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piBxWRUTT45z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "tt =pd.read_csv(\"/content/gdrive/My Drive/WIDER/training_all_WIDER.csv\")\n",
        "#tt.head()\n",
        "tt.columns = [\"filename\",\"boxes\",\"Images\"]\n",
        "#tt.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMNjRm8QUDZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vv =pd.read_csv(\"/content/gdrive/My Drive/WIDER/val_file_dataframe.csv\")\n",
        "#vv.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3_V6pcGUPnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAu04dXaUsyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = ImageDataGenerator(rescale = 1./255.,)\n",
        "val_gen = ImageDataGenerator(rescale = 1./255.,)\n",
        "test_gen = ImageDataGenerator(rescale = 1./255.,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwKY5KZT8vaM",
        "colab_type": "code",
        "outputId": "8bcb0ed0-4533-4fb6-ae31-2eefff483141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator = test_gen.flow_from_directory(directory=\"/content/data/WIDER_test/images\",shuffle=True, batch_size=10,target_size=(416,416),class_mode =None, classes=None)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16097 images belonging to 61 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sWxXkZXR0hm",
        "colab_type": "code",
        "outputId": "02d50845-bb15-43c6-fd52-2af413aa2db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_generator_27 = train_gen.flow_from_dataframe(dataframe=tt,directory=\"/content/data/WIDER_train/images\",shuffle=False, xcol =0,batch_size=27,target_size=(416,416),class_mode =None, )\n",
        "val_generator_25 = val_gen.flow_from_dataframe(dataframe=vv,directory=\"/content/data/WIDER_val/images\",shuffle=False, xcol =0,batch_size=25,target_size=(416,416),class_mode =None, )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12879 images.\n",
            "Found 3225 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R4uIrfsqBE_",
        "colab_type": "code",
        "outputId": "2c2fd6d3-788e-4662-ca39-ce284499cc84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_generator_3 = train_gen.flow_from_dataframe(dataframe=tt,directory=\"/content/data/WIDER_train/images\",shuffle=False, xcol =0,batch_size=3,target_size=(416,416),class_mode =None, )\n",
        "val_generator_3 = val_gen.flow_from_dataframe(dataframe=vv,directory=\"/content/data/WIDER_val/images\",shuffle=False, xcol =0,batch_size=3,target_size=(416,416),class_mode =None, )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12879 images.\n",
            "Found 3225 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfOLmm1lU2zc",
        "colab_type": "code",
        "outputId": "2dafd2f4-64e4-4df7-db4e-fbd20f466936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_generator_9 = train_gen.flow_from_dataframe(dataframe=tt,directory=\"/content/data/WIDER_train/images\",shuffle=False, xcol =0,batch_size=9,target_size=(416,416),class_mode =None, )\n",
        "val_generator_5 = val_gen.flow_from_dataframe(dataframe=vv,directory=\"/content/data/WIDER_val/images\",shuffle=False, xcol =0,batch_size=5,target_size=(416,416),class_mode =None, )\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12879 images.\n",
            "Found 3225 images.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaTpvvrKVT07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_tot_gen(genX1,genX2,genX3,genX4):\n",
        "  while True:\n",
        "    X1 = genX1.__next__()\n",
        "    X2 = genX2.__next__()\n",
        "    X3 = genX3.__next__()\n",
        "    X4 = genX4.__next__()\n",
        "    y = np.zeros(len(X1))\n",
        "    yield [X1, X2,X3,X4], y\n",
        "    \n",
        "    \n",
        "def val_tot_gen(genX1,genX2,genX3,genX4):\n",
        "  while True:\n",
        "    X1 = genX1.__next__()\n",
        "    X2 = genX2.__next__()\n",
        "    X3 = genX3.__next__()\n",
        "    X4 = genX4.__next__()\n",
        "    y = np.zeros(len(X1))\n",
        "\n",
        "    \n",
        "    yield [X1, X2,X3,X4],y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Xa_fxBVFFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(anchors, class_names, load_pretrained=True, freeze_body=True):\n",
        "    '''\n",
        "    returns the body of the model and the model\n",
        "    # Params:\n",
        "    load_pretrained: whether or not to load the pretrained model or initialize all weights\n",
        "    freeze_body: whether or not to freeze all weights except for the last layer's\n",
        "    # Returns:\n",
        "    model_body: YOLOv2 with new output layer\n",
        "    model: YOLOv2 with custom loss Lambda layer\n",
        "    '''\n",
        "\n",
        "    detectors_mask_shape = (13, 13, 5, 1)\n",
        "    matching_boxes_shape = (13, 13, 5, 5)\n",
        "\n",
        "    # Create model input layers.\n",
        "    image_input = Input(shape=(416, 416, 3))\n",
        "    boxes_input = Input(shape=(None, 5))\n",
        "    detectors_mask_input = Input(shape=detectors_mask_shape)\n",
        "    matching_boxes_input = Input(shape=matching_boxes_shape)\n",
        "\n",
        "    # Create model body.\n",
        "    yolo_model = yolo_body(image_input, len(anchors), len(class_names))\n",
        "    topless_yolo = Model(yolo_model.input, yolo_model.layers[-2].output)\n",
        "\n",
        "    if load_pretrained:\n",
        "        # Save topless yolo:\n",
        "        topless_yolo_path = os.path.join('/content', 'yolo_topless.h5')\n",
        "        if not os.path.exists(topless_yolo_path):\n",
        "            print(\"CREATING TOPLESS WEIGHTS FILE\")\n",
        "            yolo_path = os.path.join('/content', 'yolo.h5')\n",
        "            model_body = load_model(yolo_path)\n",
        "            model_body = Model(model_body.inputs, model_body.layers[-2].output)\n",
        "            model_body.save_weights(topless_yolo_path)\n",
        "        topless_yolo.load_weights(topless_yolo_path)\n",
        "\n",
        "    if freeze_body:\n",
        "        for layer in topless_yolo.layers:\n",
        "            layer.trainable = False\n",
        "    final_layer = Conv2D(len(anchors)*(5+len(class_names)), (1, 1), activation='linear')(topless_yolo.output)\n",
        "\n",
        "    model_body = Model(image_input, final_layer)\n",
        "\n",
        "    # Place model loss on CPU to reduce GPU memory usage.\n",
        "    with tf.device('/cpu:0'):\n",
        "        # TODO: Replace Lambda with custom Keras layer for loss.\n",
        "        model_loss = Lambda(\n",
        "            yolo_loss,\n",
        "            output_shape=(1, ),\n",
        "            name='yolo_loss',\n",
        "            arguments={'anchors': anchors,\n",
        "                       'num_classes': len(class_names)})([\n",
        "                           model_body.output, boxes_input,\n",
        "                           detectors_mask_input, matching_boxes_input\n",
        "                       ])\n",
        "\n",
        "    model = Model(\n",
        "        [model_body.input, boxes_input, detectors_mask_input,\n",
        "         matching_boxes_input], model_loss)\n",
        "\n",
        "    return model_body, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQeBbS-TVQZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, class_names, anchors, trainimg,valimg, trainboxes,valboxes,\n",
        "          train_detectors_mask, val_detectors_mask,\n",
        "          train_matching_true_boxes,val_matching_true_boxes):\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss={\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
        "        },)  # This is a hack to use the custom loss function in the last layer.\n",
        "\n",
        "\n",
        "    logging = TensorBoard()\n",
        "    filepath=\"/gdrive/My\\ Drive/WIDER/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,save_weights_only = True,)\n",
        "    #checkpoint = ModelCheckpoint(\"trained_stage_3_best.h5\", monitor='val_loss',\n",
        "    #                             save_weights_only=True, save_best_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
        "\n",
        "    model.fit_generator(train_tot_gen(trainimg, trainboxes, train_detectors_mask, train_matching_true_boxes),\n",
        "                        steps_per_epoch=(12879//27),\n",
        "                        validation_data = val_tot_gen(valimg,valboxes,val_detectors_mask,val_matching_true_boxes),\n",
        "                        validation_steps = 3225//25,epochs=5,callbacks=[logging])\n",
        "    model.save_weights('trained_stage_1.h5')\n",
        "    model.save_weights('/grive/My\\ Drive/WIDER/trained_stage_1_1.h5')\n",
        "\n",
        "  \n",
        "\n",
        "def draw(model_body, class_names, anchors, image_data, image_set='val',\n",
        "            weights_name='trained_stage_3_best.h5', out_path=\"output_images\", save_all=True):\n",
        "   \n",
        "    if image_set == 'train':\n",
        "        image_data = np.array([np.expand_dims(image, axis=0)\n",
        "            for image in image_data[:int(len(image_data)*.9)]])\n",
        "    elif image_set == 'val':\n",
        "        image_data = np.array([np.expand_dims(image, axis=0)\n",
        "            for image in image_data[int(len(image_data)*.9):]])\n",
        "    elif image_set == 'all':\n",
        "        image_data = np.array([np.expand_dims(image, axis=0)\n",
        "            for image in image_data])\n",
        "    else:\n",
        "        ValueError(\"draw argument image_set must be 'train', 'val', or 'all'\")\n",
        "    # model.load_weights(weights_name)\n",
        "    print(image_data.shape)\n",
        "    model_body.load_weights(weights_name)\n",
        "\n",
        "    # Create output variables for prediction.\n",
        "    yolo_outputs = yolo_head(model_body.output, anchors, len(class_names))\n",
        "    input_image_shape = K.placeholder(shape=(2, ))\n",
        "    boxes, scores, classes = yolo_eval(\n",
        "        yolo_outputs, input_image_shape, score_threshold=0.07, iou_threshold=0.0)\n",
        "\n",
        "    # Run prediction on overfit image.\n",
        "    sess = K.get_session()  # TODO: Remove dependence on Tensorflow session.\n",
        "\n",
        "    if  not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "    for i in range(len(image_data)):\n",
        "        out_boxes, out_scores, out_classes = sess.run(\n",
        "            [boxes, scores, classes],\n",
        "            feed_dict={\n",
        "                model_body.input: image_data[i],\n",
        "                input_image_shape: [image_data.shape[2], image_data.shape[3]],\n",
        "                K.learning_phase(): 0\n",
        "            })\n",
        "        print('Found {} boxes for image.'.format(len(out_boxes)))\n",
        "        print(\"Boxes:\",out_boxes)\n",
        "        \n",
        "\n",
        "        # Plot image with predicted boxes.\n",
        "        image_with_boxes = draw_boxes_(image_data[i][0], out_boxes, out_classes,\n",
        "                                    class_names, out_scores)\n",
        "        print(\"--------------------------------------------\")\n",
        "        # Save the image:\n",
        "        if save_all or (len(out_boxes) > 0):\n",
        "             image = PIL.Image.fromarray(image_with_boxes)\n",
        "             image.save(os.path.join(out_path,str(i)+'.png'))\n",
        "            #cv2.imwrite(os.path.join(out_path,str(i)+'.png'),image_with_boxes)\n",
        "        # To display (pauses the program):\n",
        "        # plt.imshow(image_with_boxes, interpolation='nearest')\n",
        "        # plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRnQWDITXlc_",
        "colab_type": "code",
        "outputId": "55c4cbed-2481-4f2c-cdba-52d80bd3311d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "classes_path = \"/content/gdrive/My Drive/WIDER/classes.txt\"\n",
        "\n",
        "class_names = get_classes(classes_path)\n",
        "#anchors = get_anchors(anchors_path)\n",
        "\n",
        "#data = np.load(data_path) # custom data saved as a numpy file.\n",
        "#  has 2 arrays: an object array 'boxes' (variable length of boxes in each image)\n",
        "#  and an array of images 'images'\n",
        "\n",
        "#image_data, boxes = process_data(data['images'], data['boxes'])\n",
        "\n",
        "anchors = YOLO_ANCHORS\n",
        "\n",
        "#detectors_mask, matching_true_boxes = get_detector_mask(boxes, anchors)\n",
        "\n",
        "model_body, model = create_model(anchors, class_names,freeze_body=False)\n",
        "\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfNGP7iOSMCw",
        "colab_type": "code",
        "outputId": "e2dce6d4-168e-49d3-9ded-a995bfb95459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "source": [
        "train(\n",
        "    model,\n",
        "    class_names,\n",
        "    anchors,\n",
        "    train_generator_27,\n",
        "    val_generator_25,\n",
        "    box_batch_train_27,\n",
        "    box_batch_val_25,\n",
        "    det_batch_train_27,\n",
        "    det_batch_val_25,\n",
        "    match_batch_train_27,\n",
        "    match_batch_val_25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "477/477 [==============================] - 1497s 3s/step - loss: 393.9393 - val_loss: 3815.0439\n",
            "Epoch 2/5\n",
            "477/477 [==============================] - 1470s 3s/step - loss: 285.2336 - val_loss: 314.7425\n",
            "Epoch 3/5\n",
            "477/477 [==============================] - 1459s 3s/step - loss: 248.5030 - val_loss: 267.6052\n",
            "Epoch 4/5\n",
            "477/477 [==============================] - 1458s 3s/step - loss: 669.7827 - val_loss: 1093.9394\n",
            "Epoch 5/5\n",
            "477/477 [==============================] - 1460s 3s/step - loss: 641.7783 - val_loss: 492.8697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-38e8978802a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdet_batch_val_25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmatch_batch_train_27\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     match_batch_val_25)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-68-d64cca9a6e0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, class_names, anchors, trainimg, valimg, trainboxes, valboxes, train_detectors_mask, val_detectors_mask, train_matching_true_boxes, val_matching_true_boxes)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         validation_steps = 3225//25,epochs=5,callbacks=[logging])\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_stage_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/grive/My\\ Drive/WIDER/trained_stage_1_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"model_body, model = create_model(anchors, class_names, load_pretrained=False, freeze_body=False)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/grive/My\\ Drive/WIDER/trained_stage_1_1.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbcmsDKe2gP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('/content/gdrive/My Drive/WIDER/trained_stage_1_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO0-NaWpYVfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train2(model, class_names, anchors, trainimg,valimg, trainboxes,valboxes,\n",
        "          train_detectors_mask, val_detectors_mask,\n",
        "          train_matching_true_boxes,val_matching_true_boxes):\n",
        "    '''\n",
        "    retrain/fine-tune the model\n",
        "    logs training with tensorboard\n",
        "    saves training weights in current directory\n",
        "    best weights according to val_loss is saved as trained_stage_3_best.h5\n",
        "    '''\n",
        "    \"\"\"model.compile(\n",
        "        optimizer=op.Adam(lr = 1e-3), loss={\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
        "        },)  # This is a hack to use the custom loss function in the last layer.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    logging = TensorBoard(log_dir = '/content/gdrive/My Drive/WIDER/logs')\n",
        "    filepath=\"/content/gdrive/My Drive/WIDER/epochs:{epoch:03d}-val_loss:{val_loss:.3f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,save_weights_only = True,)\n",
        "    #checkpoint = ModelCheckpoint(\"trained_stage_3_best.h5\", monitor='val_loss',\n",
        "    #                             save_weights_only=True, save_best_only=True)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
        "\n",
        "    \"\"\"model.fit_generator(train_tot_gen(trainimg, trainboxes, train_detectors_mask, train_matching_true_boxes),\n",
        "                        steps_per_epoch=(12879//27),\n",
        "                        validation_data = val_tot_gen(valimg,valboxes,val_detectors_mask,val_matching_true_boxes),\n",
        "                        validation_steps = 3225//25,epochs=5,callbacks=[logging])\n",
        "    model.save_weights('trained_stage_1.h5')\"\"\"\n",
        "\n",
        "    model_body, model = create_model(anchors, class_names, load_pretrained=False, freeze_body=False)\n",
        "\n",
        "    model.load_weights('/content/gdrive/My Drive/WIDER/fin_epochs:042-val_loss:35.310.hdf5')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=op.Adam(lr = 1e-5), loss={\n",
        "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
        "        })  # This is a hack to use the custom loss function in the last layer.\n",
        "    model.fit_generator(train_tot_gen(trainimg, trainboxes, train_detectors_mask, train_matching_true_boxes),\n",
        "                        steps_per_epoch=(12879//3),\n",
        "                        validation_data = val_tot_gen(valimg,valboxes,val_detectors_mask,val_matching_true_boxes),\n",
        "                        validation_steps = 3225//3,epochs=60,callbacks=[logging,checkpoint],initial_epoch =41)\n",
        "\n",
        "    \"\"\"model.fit_generator(tot_gen(image_data, boxes, detectors_mask, matching_true_boxes),\n",
        "              np.zeros(len(image_data)),\n",
        "              steps_per_epoch=12879/32,\n",
        "              #batch_size=8,\n",
        "              epochs=30,\n",
        "              callbacks=[logging])\"\"\"\n",
        "\n",
        "    model.save_weights('trained_stage_2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBK3-m95LSEi",
        "colab_type": "code",
        "outputId": "ece642f0-5412-4103-965e-8107f8db7948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "train2(\n",
        "    model,\n",
        "    class_names,\n",
        "    anchors,\n",
        "    train_generator_9,\n",
        "    val_generator_5,\n",
        "    box_batch_train_9,\n",
        "    box_batch_val_5,\n",
        "    det_batch_train_9,\n",
        "    det_batch_val_5,\n",
        "    match_batch_train_9,\n",
        "    match_batch_val_5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/60\n",
            "1431/1431 [==============================] - 2213s 2s/step - loss: 60.3533 - val_loss: 51.1444\n",
            "\n",
            "Epoch 00042: val_loss improved from inf to 51.14437, saving model to /content/gdrive/My Drive/WIDER/epochs:042-val_loss:51.144.hdf5\n",
            "Epoch 43/60\n",
            "1431/1431 [==============================] - 2241s 2s/step - loss: 57.7232 - val_loss: 51.9534\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 51.14437\n",
            "Epoch 44/60\n",
            "1431/1431 [==============================] - 2244s 2s/step - loss: 56.2754 - val_loss: 52.5175\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 51.14437\n",
            "Epoch 45/60\n",
            "1431/1431 [==============================] - 2244s 2s/step - loss: 55.1924 - val_loss: 52.7456\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 51.14437\n",
            "Epoch 46/60\n",
            "1431/1431 [==============================] - 2247s 2s/step - loss: 54.3117 - val_loss: 53.0587\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 51.14437\n",
            "Epoch 47/60\n",
            "1431/1431 [==============================] - 2244s 2s/step - loss: 53.5820 - val_loss: 53.7138\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 51.14437\n",
            "Epoch 48/60\n",
            " 532/1431 [==========>...................] - ETA: 21:25 - loss: 73.8916"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3ae44462da3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdet_batch_val_5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmatch_batch_train_9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     match_batch_val_5)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-d86535bb1c3e>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(model, class_names, anchors, trainimg, valimg, trainboxes, valboxes, train_detectors_mask, val_detectors_mask, train_matching_true_boxes, val_matching_true_boxes)\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12879\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_tot_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_detectors_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_matching_true_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         validation_steps = 3225//5,epochs=60,callbacks=[logging,checkpoint],initial_epoch =41)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"model.fit_generator(tot_gen(image_data, boxes, detectors_mask, matching_true_boxes),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8VOrEzGm9Ns",
        "colab_type": "code",
        "outputId": "d4d39f93-676f-4d7b-b4a2-2f619908e2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "train2(\n",
        "    model,\n",
        "    class_names,\n",
        "    anchors,\n",
        "    train_generator_3,\n",
        "    val_generator_3,\n",
        "    box_batch_train_3,\n",
        "    box_batch_val_3,\n",
        "    det_batch_train_3,\n",
        "    det_batch_val_3,\n",
        "    match_batch_train_3,\n",
        "    match_batch_val_3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 42/60\n",
            "4293/4293 [==============================] - 2904s 677ms/step - loss: 25.4733 - val_loss: 34.6233\n",
            "\n",
            "Epoch 00042: val_loss improved from inf to 34.62326, saving model to /content/gdrive/My Drive/WIDER/epochs:042-val_loss:34.623.hdf5\n",
            "Epoch 43/60\n",
            "4293/4293 [==============================] - 2913s 679ms/step - loss: 25.1141 - val_loss: 34.5941\n",
            "\n",
            "Epoch 00043: val_loss improved from 34.62326 to 34.59413, saving model to /content/gdrive/My Drive/WIDER/epochs:043-val_loss:34.594.hdf5\n",
            "Epoch 44/60\n",
            "4293/4293 [==============================] - 2891s 673ms/step - loss: 24.8764 - val_loss: 34.4761\n",
            "\n",
            "Epoch 00044: val_loss improved from 34.59413 to 34.47607, saving model to /content/gdrive/My Drive/WIDER/epochs:044-val_loss:34.476.hdf5\n",
            "Epoch 45/60\n",
            "4293/4293 [==============================] - 2863s 667ms/step - loss: 24.6799 - val_loss: 34.4815\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 34.47607\n",
            "Epoch 46/60\n",
            "4293/4293 [==============================] - 2869s 668ms/step - loss: 24.5017 - val_loss: 34.6571\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 34.47607\n",
            "Epoch 47/60\n",
            "1399/4293 [========>.....................] - ETA: 29:47 - loss: 33.6840"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-a1eecc9f7e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdet_batch_val_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmatch_batch_train_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     match_batch_val_3)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-b9e19f2cdc43>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(model, class_names, anchors, trainimg, valimg, trainboxes, valboxes, train_detectors_mask, val_detectors_mask, train_matching_true_boxes, val_matching_true_boxes)\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12879\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_tot_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_detectors_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_matching_true_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         validation_steps = 3225//3,epochs=60,callbacks=[logging,checkpoint],initial_epoch =41)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"model.fit_generator(tot_gen(image_data, boxes, detectors_mask, matching_true_boxes),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOWxUO_m0QX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = test_generator.next()\n",
        "draw(model_body,\n",
        "class_names,\n",
        "anchors,\n",
        "x,\n",
        "image_set='all', # assumes training/validation split is 0.9\n",
        "weights_name='/content/gdrive/My Drive/WIDER/epochs:042-val_loss:35.310.hdf5',\n",
        "save_all=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riDP_3o3_ueh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}